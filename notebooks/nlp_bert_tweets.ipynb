{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a3afa7-f90a-492a-bcf5-13aa07c26e02",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ae9ad8-2939-4007-bee2-33f216386291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
    "import string\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab806e4-0cae-4810-ae4d-e2a20d5fc18e",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37dd9f9c-59ae-4cf6-bbd0-cb9b84e93574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e8b14d-c648-43fb-8395-cf3bb58d9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_axis([\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b88daea-10f0-429b-8a39-e152c74dba56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f749293-1a29-4766-9d02-5b6b9c1105c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e708507-2aaa-4366-9295-a4440c52861c",
   "metadata": {},
   "source": [
    "# Pre-processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a5e43-d900-4346-9f2d-d45579ffa67f",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a71fcb2-ba8e-4fef-9739-ee9de057ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22688eb-92a7-487f-a5df-a88b98a804e7",
   "metadata": {},
   "source": [
    "## Replace emojis with text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ce6b8-d1ae-4ee2-904e-efba286d4409",
   "metadata": {},
   "source": [
    "https://www.oreilly.com/library/view/python-cookbook/0596001673/ch03s15.html\n",
    "\n",
    "https://medium.com/geekculture/text-preprocessing-how-to-handle-emoji-emoticon-641bbfa6e9e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7941d1e2-9c1d-4bb9-8c43-41355fa7d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Emoji_Dict.p'- download link https://drive.google.com/open?id=1G1vIkkbqPBYPKHcQ8qy0G2zkoab2Qv4v\n",
    "\n",
    "\n",
    "with open('Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "# def convert_emojis_to_word(text):\n",
    "#     regex = re.compile(\"|\".join(map(re.escape, Emoji_Dict.keys(  ))))\n",
    "#     # For each match, look up the corresponding value in the dictionary\n",
    "#     text = regex.sub(lambda match: \" \".join(Emoji_Dict[match.group(0).replace(\"_\",\" \").replace(\",\",\"\").replace(\":\",\"\").split()]), text)\n",
    "#     text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "#     # for emot in Emoji_Dict:\n",
    "#     #     text = re.sub(emot,r' '+emot, text)\n",
    "#     #     text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "#     #     text = re.sub(r'('+emot+')', \" \".join(Emoji_Dict[emot].replace(\"_\",\" \").replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "#     return text\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    # print(text)\n",
    "    regex = re.compile(\"|\".join(map(re.escape, Emoji_Dict.keys(  ))))\n",
    "    # print(regex.sub(lambda match: Emoji_Dict[match.group(0)], text))\n",
    "    # print(regex.sub(lambda match: Emoji_Dict[match.group(0)].replace(\"_\",\" \").replace(\",\",\" \").replace(\":\",\" \"), text))\n",
    "    text = regex.sub(lambda match: Emoji_Dict[match.group(0)].replace(\"_\",\" \").replace(\",\",\" \").replace(\":\",\" \"), text)\n",
    "    # text = regex.sub(lambda match: \" \".join(Emoji_Dict[match.group(0)].replace(\"_\",\" \").replace(\",\",\" \").replace(\":\",\" \").split()), text)\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(convert_emojis_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4f7f5-9128-4f10-b42c-d8d497be6160",
   "metadata": {},
   "source": [
    "## Replace abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d15149-b0af-4161-9ec6-145594adc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    \n",
    "    'shoulda': 'should have',\n",
    "    'gonna': 'going to', \n",
    "    'wanna': 'wanting to',\n",
    "    \"ain't\": \"is not\",\n",
    "    \"wana\": 'wanting to',\n",
    "   \n",
    "    \n",
    "    'ngl': 'not going to lie',\n",
    "    'idk': 'i do not know',\n",
    "    'fyi': 'for your information',\n",
    "    'tbh': 'to be honest',\n",
    "    'asap': 'as soon as possible',\n",
    "    'bbiab': 'be back in a bit',\n",
    "    'bbl': 'be back later',\n",
    "    'bbs': 'be back soon',\n",
    "    'bf': 'boyfriend',\n",
    "    'bff': 'best friend forever',\n",
    "    'brb': 'be right back',\n",
    "    'cya': 'see you',\n",
    "    'faq': 'frequently asked questions',\n",
    "    'ftw': 'for the win',\n",
    "    'g2g': 'got to go',\n",
    "    'gf': 'girlfriend',\n",
    "    'gr8': 'great',\n",
    "    'hru': 'how are you',\n",
    "    'ight': 'alright',\n",
    "    'imo': 'in my opinion',\n",
    "    'imy': 'i miss you',\n",
    "    'irl': 'in real life',\n",
    "    'istg': 'i swear',\n",
    "    'lmao': 'laughing',\n",
    "    'lmk': 'let me know',\n",
    "    'lol': 'laughing',\n",
    "    'nvd': 'nevermind',\n",
    "    'noob': 'amateur',\n",
    "    ' np ': ' no problem ',\n",
    "    'ofc': 'of course',\n",
    "    'omg': 'i can not believe it',\n",
    "    'rn': 'right now',\n",
    "    'ttyl': 'talk to you later',\n",
    "    ' u ': ' you ',\n",
    "    'wym': 'what do you mean ?',\n",
    "    ' y ': ' why ',\n",
    "    'yw': 'you are welcome'\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def replace_abbreviations(text):\n",
    "    regex = re.compile(\"|\".join(map(re.escape, abbr_dict.keys(  ))))\n",
    "    text = regex.sub(lambda match: abbr_dict[match.group(0)], text)\n",
    "    return text\n",
    "df[\"text\"] = df[\"text\"].apply(replace_abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3bd3b-d91d-4578-b328-3c75e9899a20",
   "metadata": {},
   "source": [
    "## Handle punctiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "205ed106-7188-485e-91a0-3623feb09fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctiations = {}\n",
    "for elem in string.punctuation:\n",
    "    punctiations[elem] = f\" {elem} \"\n",
    "punctiations[\"...\"] = \" ... \"\n",
    "def handle_punctiation(text):\n",
    "    regex = re.compile(\"|\".join(map(re.escape, punctiations.keys(  ))))\n",
    "    text = regex.sub(lambda match: punctiations[match.group(0)], text)\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "    return text\n",
    "df[\"text\"] = df[\"text\"].apply(handle_punctiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9304fe9-c30b-41ec-9f41-4b1afa8bfbda",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5dc8554-b2ff-474a-8c3c-5044cef8bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_preprocess(x):\n",
    "    # Delete links\n",
    "    x = \" \".join(filter(lambda y: not(y.startswith(\"http\")), x.split()))\n",
    "    x = \" \".join(filter(lambda y: not(y.endswith(\".com\")), x.split()))\n",
    "    \n",
    "    # Remove stop words\n",
    "    x = \" \".join(filter(lambda y: not(y in stopwords.words('english')), x.split()))\n",
    "    \n",
    "    x = \" \".join(filter(lambda y: y[0]!=\"@\", x.split()))\n",
    "    \n",
    "    return x\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(filter_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38796f-f50b-4db3-977a-b65e281d0661",
   "metadata": {},
   "source": [
    "# Tokenize and encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c2bb489-f0e2-4835-94e1-2645d32cbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "def75d00-8379-4d2c-900a-12a0c935db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all different words\n",
    "l = set(df[\"text\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27248edd-9644-4c4d-accc-b1ffe0afd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = {k: v for v, k in enumerate(l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201a54c7-4870-4611-9a78-0f981fdce7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode words thanks to vocab\n",
    "def voc_encode(x):\n",
    "    try:\n",
    "        return vocab[x]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def encode(x):\n",
    "    return list(map(voc_encode, x))\n",
    "df[\"text\"] = df[\"text\"].apply(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130620b-b858-4698-b6f1-43e75284f6b2",
   "metadata": {},
   "source": [
    "# Handle label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7be5d85d-cf91-4bbc-a353-e3524246c409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50066\n",
       "4    49934\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69667c16-ad87-4b69-908f-db7b94ef76ee",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce07b4d2-41bf-47e5-aa36-76599d733b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f66b79-6db9-4205-a7b0-62519900def0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SnowballStemmer' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2332/1477959755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menglishStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SnowballStemmer' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "englishStemmer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d46a134-31d3-4ab8-aff4-d402525fb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3962812e-1cda-4e7b-a12e-3433657ace9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting emot\n",
      "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: emot\n",
      "Successfully installed emot-3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install emot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
